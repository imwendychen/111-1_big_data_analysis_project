#homework2-MIS572-B094020007
library(caret)
library(ggplot2)
library(glmnet)
library(Matrix)
library(randomForest)
library(data.table)
library(pROC)
library(rpart)
library(rpart.plot)
###QUESTION1
#Please load the given dataset “redwinequality.csv” and answer the following data management questions.
rwq<-read.csv("~/Desktop/大學/大三上/巨量/redwinequality.csv")

##1.1----------- 
#Please split the data into training set (70%) and testing set (30%), make sure you set "set.seed(2022)". 
#Fit linear regression models that predict "quality" and calculate Mean Absolute Error(MAE) for both training and testing data.

set.seed(2022)
idx <- sample(1:nrow(rwq), nrow(rwq) * 0.7)
train_idx <- rwq[idx,]
test_idx <- rwq[-idx,]
lm_fit <- lm(quality ~ . , train_idx)
summary(lm_fit)

MAE <- function(predict, actual) 
  return(mean(abs(predict - actual)))

MAE(lm_fit$fitted.values, train_idx$quality)  
# MAE of train_idx=0.4914
lm_predict <- predict(lm_fit, test_idx)
MAE(lm_predict, test_idx$quality)  
# MAE of test_idx=0.521

##1.2----------- 
#Please use forward and backward selections to find your best subsets of variables (up to two-way interactions). 
#Did these selections choose the same variables?
#Also, report the training/testing MAEs of with variables selected by each method.
nullModel <- lm(quality ~ 1, data=train_idx)
fullModel <- lm(quality ~ (.)^2, data=train_idx)

#forward selection
forward <- step(nullModel, scope = list(lower = nullModel, upper = fullModel), direction = 'forward')
MAE(forward$fitted.values, train_idx$quality)
#train_idx MAE=0.4825258
predict_forward <- predict(forward, test_idx)
MAE(predict_forward, test_idx$quality)
#test_idx MAE=0.5063145

#backward selection
backward <- step(fullModel, scope = list(upper = fullModel), direction = 'backward')
MAE(backward$fitted.values, train_idx$quality)
#train_idx MAE=0.475894
predict_backward <- predict(backward, test_idx)
MAE(predict_backward, test_idx$quality)  
# test_idx MAE=0.5100406

# No, these 2 selections select different variables

##1.3----------- 
#Refer to Chapter 6 (Linear Model Selection and Regularization) of the textbook (ISLR). 
#Please fit a LASSO with all variables and R package “glmnet”, 
#use cv.glmnet() to find the best lambda. 
#Also report the training/testing MAEs.
cv_lasso = cv.glmnet(x = as.matrix(train_idx[, -12]), 
                     y = train_idx[, 12], 
                     alpha = 1,  
                     # 0 for ridge ; 1 for lasso
                     family = "gaussian")
best_lambda = cv_lasso$lambda.min
best_lambda 
#best lambda=0.0006283377

# LASSO regression w/ best lambda
lasso_m = glmnet(x = as.matrix(train_idx[, -12]), 
                 y = train_idx[, 12], 
                 alpha = 1,  
                 # 0 for ridge ; 1 for lasso
                 family = "gaussian", lambda = best_lambda)

predicted = predict(lasso_m, as.matrix(train_idx[, -12]) )
MAE(predicted, train_idx$quality) 

#train_idx MAE=0.4915345

predicted = predict(lasso_m, as.matrix(test_idx[, -12]) )
MAE(predicted, test_idx$quality) 
#test_idx MAE=0.5216385

##1.4----------- 
#Fit a random forest with all variables and report training/testing MAEs. 
#Print out the importance of variance of random forest and order by the importance. 
#Are random forest rankings different from those generated by linear models?
rf_fit <- randomForest(quality ~ ., train_idx)
predicted = predict(rf_fit, train_idx)
MAE(predicted, train_idx$quality)

#train_idx MAE=0.19357

predicted = predict(rf_fit, test_idx)
MAE(predicted, test_idx$quality) 

#test_idx MAE=0.44371

#variable importance ranking of RandomForest model
importance(rf_fit)[order(importance(rf_fit), decreasing = T),]
#variable importance ranking of linear models
caret::varImp(forward)
caret::varImp(backward)
#the rankings are differ from each other.

###QUESTION2
##2.1----------- 
#Split the data into training (70%) and testing (30%) datasets with set.seed(2022). 
#Fit a Logistic regression model to predict “Outcome” and report both the training and testing AUC.

diabetes <- fread("~/Desktop/大學/大三上/巨量/diabetes.csv")
diabetes$Outcome = as.factor(diabetes$Outcome)

#split
set.seed(2022)
index <-  sample(1:nrow(diabetes), nrow(diabetes) * 0.7)
train_idx <- diabetes[index, ]
test_idx <- diabetes[-index, ]
lm_diabetes = glm(Outcome ~ ., data = train_idx, family = "binomial")

round(auc(train_idx$Outcome, predict(lm_diabetes, train_idx, type = 'response')), 4)

# train_idx AUC: 0.8458

round(auc(test_idx$Outcome, predict(lm_diabetes, newdata = test_idx, type = 'response')), 4)

# test_idx AUC: 0.8127


#2.2----------- 
#Tree-based algorithms, such as CART, are able to identify non-linear relationships from data and 
#represent the relationships in interpretable rules. 
#Please use R package “rpart” and “rpart.plot” to fit a classification tree” that predict “Outcome”. 
#Select and plot your best tree. Then report both the training and testing AUC.

#Fit a CART with all variables by R function rpart(). 
rpart_diabetes <- rpart(Outcome ~ ., data = train_idx)
rpart.plot(rpart_diabetes, type = 1, tweak = 2)
printcp(rpart_diabetes)

#select a tree which is nsplit = 11 that balances the error of cross-validation and information given by tree.

#Calculate AUC of predictions to train_idx & test_idx

round(auc(train_idx$Outcome, predict(rpart_diabetes)[, 2]), 4)
# train_idx AUC=0.868
round(auc(test_idx$Outcome, predict(rpart_diabetes, newdata = test_idx)[, 2]), 4)
# test_idx AUC=0.756

##2.3
#Ensemble learning methods like random forest usually outperform the other simple models alone in terms of accuracy 
#of prediction, because they reduce the variance of prediction by aggregating multiple models. 
#Please fit a random forest model and report training/testing AUC.

#fit a random forest with all of the variables. 
rf_diabetes <- randomForest(Outcome ~ ., ntree = 500,  data = train_idx)
rf_diabetes
# Call:
#   randomForest(formula = Outcome ~ ., data = train_idx, ntree = 500) 
# Type of random forest: classification
# Number of trees: 500
# No. of variables tried at each split: 2
# 
# OOB estimate of  error rate: 22.35%
# Confusion matrix:
#   0   1 class.error
# 0 304  49   0.1388102
# 1  71 113   0.3858696

#calculate AUC of predictions to train_idx & test_idx

round(auc(train_idx$Outcome, predict(rf_diabetes, type = "prob")[, 2]), 4) 
#train_idx AUC: 0.8342
round(auc(test_idx$Outcome, predict(rf_diabetes, newdata = test_idx, type = "prob")[, 2]), 4)
#test_idx AUC: 0.8025

# 2.4
#Which variable you believe is the most important to predict the “Outcome”? 
#Justify your finding with your analysis result. 

#print the variable importance ranking of randomforest & CART

importance(rf_diabetes)[order(importance(rf_diabetes), decreasing = T),]
rpart_diabetes$variable.importance

#Glucose seems like the most vital variable.

###QUESTION3
#Please load the given data body signal of smoking Dataset (smoking.csv). 
#Refer to here for more information.

##3.1----------- 
#Use R package caret to split the data into training (70%) and testing (30%) datasets 
#with set.seed(2022). Fit Logistic regression models that predict “smoking”. Use or create
#any variables that may better predict the target. What are the accuracy of predictions on 
#both the training and the testing datasets, given the default cutoff value 0.5?

smoke <-  fread('~/Desktop/大學/大三上/巨量/smoking.csv', data.table = F, stringsAsFactors = T)
smoke <- smoke[, -c(1, 24)] #Remove ID and oral, because oral has only one level.
smoke[, c(1,  24:25)] <- lapply(smoke[, c(1, 24:25)], as.factor)

set.seed(2022)
index <- createDataPartition(smoke$smoking, p = .7, list = F)
train_idx <- smoke[index, ]
test_idx <-  smoke[-index, ]

#check whether there's class imbalance problem.
prop.table(table(train_idx$smoking))

#result:
# 0         1 
# 0.6327049 0.3672951 
lm <- glm(smoking ~ ., data = train_idx, family = 'binomial')
summary(lm)

predtrain <- predict(lm, train_idx, type = 'response')
predlatrain <- ifelse(predtrain > 0.5 , 1, 0) |>  as.factor()
confusionMatrix(predlatrain, train_idx$smoking, positive = "1")
#train_idx accuracy: 0.7445

predtest <- predict(lm, test_idx, type = 'response')
predlatest <- ifelse(predtest > 0.5 , 1, 0) |>  as.factor()
confusionMatrix(predlatest, test_idx$smoking,  positive = "1")
#test_idx accuracy: 0.7485

#3.2----------- 
#We can see that there is a class imbalance problem with the target (smoking). 
#We understand that adjusting predicted class probability cutoff may help predict the rare cases. 
#What is the optimal cutoff value based on Youden's J index? 
#Please also report your model True Positive Rates (Sensitivities) with different cutoff values (0.5 and the “optimal” value).

confusionMatrix(predlatrain, train_idx$smoking, positive = "1")#cutoff=0.5
#positive rate (sensitivity)=0.6940

# Optimal cutoff using Youden's J index
roc <- roc(train_idx$smoking, predtrain);roc
#train AUC=0.8302
plot.roc(roc, print.auc = T, print.thres = T, thres.best.method = 'youden')
coords(roc, "best")

#   threshold specificity sensitivity
# 1 0.3938786   0.6297332   0.8987359
# Optimal cutoff = 0.3938786

pred_label <- ifelse(predtrain > 0.3938786 , 1, 0) |>  as.factor()
caret::confusionMatrix(pred_label, train_idx$smoking,  positive = "1")

# positive rate (sensitivity)=0.8987

## 3.3----------- 

#Plot the ROC curves of your models for both training and testing datasets. 
#Compare and report your model performance in terms of AUCs.

#ROC w/ 3 cutoffs

roc(train_idx$smoking, predtrain)
plot.roc(roc(train_idx$smoking, predtrain), print.thres = c(0.8, 0.5, 0.1), print.auc = T, xlim = c(1, 0))

#train AUC=0.8302

plot.roc(roc(test_idx$smoking, predtest), print.thres = c(0.8, 0.5, 0.1), print.auc = T, xlim = c(1, 0))

#test AUC=0.83


